from typing import Dict
import torch
from attr.validators import max_len
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig
from .Finetuner import Finetuner
from peft import LoraConfig, TaskType, get_peft_model
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig


class Seq2SeqFinetuner(Finetuner):

    def __init__(self, model_name: str, compute_specs="low_end") -> None:
        super().__init__(model_name, compute_specs)
        self.task = TaskType.SEQ_2_SEQ_LM

    @staticmethod
    def format_example(example: dict, specs: str) -> Dict | None:
        # Concatenate the context, question, and answer into a single text field.
        if specs == "low_end":
            return {
                "text": f'''
                    ["role": "system", "content": "You are a text summarization assistant."],
                    [role": "user", "content": {example['article']}],
                    ["role": "assistant", "content": {example['summary']}]
                '''
            }
        elif specs == "mid_range":
            return {
                "text": f'''
                            ["role": "system", "content": "You are a text summarization assistant."],
                            [role": "user", "content": {example['article']}],
                            ["role": "assistant", "content": {example['summary']}]
                        '''
            }
        elif specs == "high_end":
            return {
                "text": f'''
                            ["role": "system", "content": "You are a text summarization assistant."],
                            [role": "user", "content": {example['article']}],
                            ["role": "assistant", "content": {example['summary']}]
                        '''
            }

    def load_dataset(self, dataset_path: str) -> None:
        dataset = load_dataset("json", data_files=dataset_path, split="train")
        dataset = dataset.map(lambda example: self.format_example(example, self.compute_specs))
        dataset = dataset.remove_columns(['article', 'summary'])
        print(dataset[0])
        self.dataset = dataset

    def set_settings(self, **kwargs) -> None:
        """
        Set model training settings from keyword arguments.
        Groups settings by category for better organization.
        """
        super().set_settings(**kwargs)


    def finetune(self) -> bool:
        print("Starting Seq2Seq fine-tuning process...")
        try:
            bits_n_bytes_config = None
            if self.use_4bit:
                bits_n_bytes_config = BitsAndBytesConfig(
                    load_in_4bit=self.use_4bit,
                    bnb_4bit_quant_type=self.bnb_4bit_quant_type,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=self.use_nested_quant,
                )
            elif self.use_8bit:
                bits_n_bytes_config = BitsAndBytesConfig(
                    load_in_8bit=self.use_8bit,
                )
            if self.use_4bit or self.use_8bit:
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name,
                    quantization_config=bits_n_bytes_config,
                    device_map=self.device_map,
                    use_cache=False,
                )
            else:
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name,
                    device_map=self.device_map,
                    use_cache=False,
                )
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            # print(tokenizer.chat_template)
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = "right"
            tokenizer.max_seq_length = tokenizer.model_max_length

            peft_config = LoraConfig(
                task_type=TaskType.SEQ_2_SEQ_LM,
                inference_mode=False,
                r=self.lora_r,
                lora_alpha=self.lora_alpha,
                lora_dropout=self.lora_dropout,
                bias="none",
            )

            training_args = SFTConfig(
                output_dir=self.output_dir,
                num_train_epochs=self.num_train_epochs,
                per_device_train_batch_size=self.per_device_train_batch_size,
                gradient_accumulation_steps=self.gradient_accumulation_steps,
                optim=self.optim,
                save_steps=self.save_steps,
                logging_steps=self.logging_steps,
                learning_rate=self.learning_rate,
                warmup_ratio=self.warmup_ratio,
                weight_decay=self.weight_decay,
                fp16=self.fp16,
                bf16=self.bf16,
                max_grad_norm=self.max_grad_norm,
                max_steps=self.max_steps,
                group_by_length=self.group_by_length,
                lr_scheduler_type=self.lr_scheduler_type,
                report_to="tensorboard",
                logging_dir=self.logging_dir,
                max_length=None
            )

            model = get_peft_model(model, peft_config)

            trainer = SFTTrainer(
                model=model,
                args=training_args,
                train_dataset=self.dataset,
            )

            trainer.train()
            trainer.model.save_pretrained(self.fine_tuned_name)
            print(f"Model saved to: {self.fine_tuned_name}")
            super().report_finish()

            return True
        except Exception as e:
            print(f"Error during fine-tuning: {e}")
            return False


        
## Unit Test
if __name__ == "__main__":
    finetuner = Seq2SeqFinetuner("google/flan-t5-base", "low_end")
    finetuner.load_dataset("C:/Users/aadit/Projects/ModelForge/ModelForge/FastAPI_server/test_datasets/low_summarization_train_set.jsonl")
    print(finetuner.dataset)