# Strategy Overview

Understanding training strategies in ModelForge v2.0.

## What Are Strategies?

**Strategies** define how models are trained:
- Model preparation (adapters, PEFT configuration)
- Dataset formatting
- Trainer setup
- Training algorithm

Different strategies offer different trade-offs in terms of memory, speed, and quality.

## Available Strategies

| Strategy | Memory | Speed | Quality | Use Case |
|----------|--------|-------|---------|----------|
| **[SFT](sft.md)** | Baseline | 1x | High | General-purpose fine-tuning |
| **[QLoRA](qlora.md)** | -30-50% | 0.9x | High | Limited VRAM |
| **[RLHF](rlhf.md)** | High | Slow | Very High | Alignment with human preferences |
| **[DPO](dpo.md)** | Medium | Medium | Very High | Simpler alternative to RLHF |

## Choosing a Strategy

### Use SFT When:

âœ… First time fine-tuning  
âœ… Have sufficient VRAM  
âœ… Standard supervised learning task  
âœ… Want simplest setup  

### Use QLoRA When:

âœ… Limited VRAM (< 12GB for 7B models)  
âœ… Want to train larger models  
âœ… Memory is the bottleneck  
âœ… Can accept slightly slower training  

### Use RLHF When:

âœ… Aligning model with human preferences  
âœ… Have reward model or feedback data  
âœ… Quality is critical  
âœ… Have computational resources  

### Use DPO When:

âœ… Have preference pairs (chosen/rejected)  
âœ… Want simpler alternative to RLHF  
âœ… Alignment without reward model  
âœ… More stable training than RLHF  

## Configuration

Specify strategy in training config:

```json
{
  "strategy": "sft"  // or "qlora", "rlhf", "dpo"
}
```

## Next Steps

- **[SFT Strategy](sft.md)** - Standard supervised fine-tuning
- **[QLoRA Strategy](qlora.md)** - Memory-efficient training
- **[Configuration Guide](../configuration/configuration-guide.md)** - All options

---

**Choose the right strategy for your needs!** ðŸŽ¯
